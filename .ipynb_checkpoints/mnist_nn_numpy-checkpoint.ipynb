{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a851b231-4bb8-4017-8caa-57167868a57b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### NumPy Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844c507c-7f42-4220-8542-2c1d7471e821",
   "metadata": {},
   "source": [
    "**Loading the MNIST dataset and preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239789f2-c1b7-4e39-9436-e6fc3f96eedc",
   "metadata": {},
   "source": [
    "Import necessary libraries. Reshape images into 1-dimensional array and normalize by dividing by 255. For labels use 1-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96107c65-9f3f-41c3-96b8-a593725162ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(x_train.shape[0], -1) / 255\n",
    "x_test = x_test.reshape(x_test.shape[0], -1) / 255\n",
    "y_train = np.eye(10)[y_train]\n",
    "y_test = np.eye(10)[y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46338625-ec82-4466-a049-bab4e3b22b67",
   "metadata": {},
   "source": [
    "**Activation functions and loss**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63551e4-f3e5-4a52-a7b8-a0a5a7988a7a",
   "metadata": {},
   "source": [
    "ReLU activation function is used for all layers, except for the output layer, where softmax function is used. As this is a multi-class classification task, cross entropy is used as a loss function. In softmax the maximal value in the array is subtracted from each element of the array for numerical stability. In cross entropy loss a small value $10^{-12}$ is added to the logarithm's argument to ensure that no -inf result occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19a2a3e9-e9af-440e-ae09-262089e906e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def softmax(x):\n",
    "    expx = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return expx / np.sum(expx, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(y, y_hat):\n",
    "    eps = 1e-12\n",
    "    return -np.mean(np.sum(y * np.log(y_hat + eps), axis=1))\n",
    "\n",
    "def cross_entropy_loss_derivative(y, y_hat):\n",
    "    return y_hat - y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5983889-5c70-43c4-a7d2-4515b692881e",
   "metadata": {},
   "source": [
    "**Initializing parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77db5359-f578-48f9-b45b-6ddcc8ba41a0",
   "metadata": {},
   "source": [
    "Define the network architecture with one input layer, three hidden layers, and one output layer. The initialize_parameters function initializes weights with He initialization and biases with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ef0ce8a-19c1-420f-8a93-0906cee0b985",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "hidden_layers = [128, 64, 32]\n",
    "output_size = 10\n",
    "\n",
    "def initialize_parameters(input_size, hidden_layers, output_size):\n",
    "    layers = [input_size] + hidden_layers + [output_size]\n",
    "    weights = []\n",
    "    biases = []\n",
    "    \n",
    "    for i in range(len(layers) - 1):\n",
    "        weights.append(np.random.randn(layers[i], layers[i + 1]) * np.sqrt(2 / layers[i]))\n",
    "        biases.append(np.zeros((1, layers[i + 1])))\n",
    "    return weights, biases\n",
    "\n",
    "weights, biases = initialize_parameters(input_size, hidden_layers, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ff513e-6dac-448f-8933-93128efe45d6",
   "metadata": {},
   "source": [
    "**Forward and backward passes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bc1bde-cc08-455c-85f9-5bcd98959f7f",
   "metadata": {},
   "source": [
    "The forward pass computes the activations for each layer in the network.\n",
    "\n",
    "For the $i$-th hidden layer compute the linear combination of inputs and weights, and add the bias:\n",
    "$$\n",
    "s_i = a_{i-1} \\cdot W_i + b_i\n",
    "$$\n",
    "Apply the ReLU activation function:\n",
    "$$\n",
    "a_i = \\text{ReLU}(s_i)\n",
    "$$\n",
    "\n",
    "For the output layer apply the softmax function instead of ReLU to obtain the final output probabilities:\n",
    "$$\n",
    "a_{\\text{out}} = \\text{softmax}(s_{\\text{out}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60e2e013-9a55-4e92-b3fa-7e845d28ee01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(x, weights, biases):\n",
    "    activations = [x]\n",
    "    s_vals = []\n",
    "    for i in range(len(weights) - 1):\n",
    "        s = np.dot(activations[-1], weights[i]) + biases[i]\n",
    "        a = relu(s)\n",
    "        activations.append(a)\n",
    "        s_vals.append(s)\n",
    "        \n",
    "    s_out = np.dot(activations[-1], weights[-1]) + biases[-1]\n",
    "    a_out = softmax(s_out)\n",
    "    activations.append(a_out)\n",
    "    s_vals.append(s_out)\n",
    "    return s_vals, activations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e4ca3d-f032-4149-8c80-8c35655e7b7b",
   "metadata": {},
   "source": [
    "Keep all the s-values and activations to be used in backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df6fba9-3709-4c59-990a-d78f18e2ea65",
   "metadata": {},
   "source": [
    "The backward pass computes the gradients of the loss with respect to weights and biases using backpropagation. \n",
    "\n",
    "Compute the gradient of the loss with respect to the output layer's activations:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial a_{\\text{out}}} = a_{\\text{out}} - y\n",
    "$$\n",
    "Let:\n",
    "$$\n",
    "\\delta_\\text{out} = \\frac{\\partial L}{\\partial a_{\\text{out}}}\n",
    "$$\n",
    "\n",
    "For the $i$-th layer, starting from the output layer and moving backward:\n",
    "\n",
    "Compute the gradients of the weights and biases:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_i} = a_{i-1}^T \\cdot \\delta_i\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b_i} = \\sum \\delta_i\n",
    "$$\n",
    "Propagate the gradient backwards to the previous layer:\n",
    "$$\n",
    "\\delta_{i-1} = (\\delta_i \\cdot W_i^T) \\cdot \\text{ReLU}'(s_{i-1})\n",
    "$$\n",
    "\n",
    "Update the weights and biases using gradient descent:\n",
    "$$\n",
    "W_i = W_i - \\eta \\cdot \\frac{\\partial L}{\\partial W_i}\n",
    "$$\n",
    "$$\n",
    "b_i = b_i - \\eta \\cdot \\frac{\\partial L}{\\partial b_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3d66ac1-2cfe-476b-822f-9e75f581d348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(y, s_vals, activations, weights, biases, learning_rate):\n",
    "    dL_da = cross_entropy_loss_derivative(y, activations[-1])\n",
    "    \n",
    "    delta = dL_da\n",
    "    \n",
    "    weight_grads = [None] * len(weights)\n",
    "    bias_grads = [None] * len(biases)\n",
    "    \n",
    "    for i in reversed(range(len(weights))):\n",
    "        weight_grads[i] = np.dot(activations[i].T, delta)\n",
    "        bias_grads[i] = np.sum(delta, axis=0, keepdims=True)\n",
    "        \n",
    "        if i > 0:\n",
    "            delta = np.dot(delta, weights[i].T) * relu_derivative(s_vals[i - 1])\n",
    "    \n",
    "    for i in range(len(weights)):\n",
    "        weights[i] -= learning_rate * weight_grads[i]\n",
    "        biases[i] -= learning_rate * bias_grads[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc7d801-712d-4c79-bb62-47e40a4ba4ad",
   "metadata": {},
   "source": [
    "**Training the neural network**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605cafec-5ee7-4bba-8b33-2872a7f35cba",
   "metadata": {},
   "source": [
    "The train function performs stochastic gradient descent with mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e26d25a-7df5-4341-bf5d-10a4f8d3196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, weights, biases, epochs, learning_rate, batch_size):\n",
    "    for epoch in range(epochs):\n",
    "        indices = np.arange(X.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        for i in range(0, X.shape[0], batch_size):\n",
    "            X_batch = X[indices[i:i + batch_size]]\n",
    "            Y_batch = Y[indices[i:i + batch_size]]\n",
    "            \n",
    "            s_vals, activations = forward_pass(X_batch, weights, biases)\n",
    "            backward_pass(Y_batch, s_vals, activations, weights, biases, learning_rate)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            s_vals, activations = forward_pass(X, weights, biases)\n",
    "            loss = cross_entropy_loss(Y, activations[-1])\n",
    "            print(f'Epoch {epoch}, Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b7bafe3-9259-4ac4-8996-b9a69f32973d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.1878581166040729\n",
      "Epoch 10, Loss: 0.01650628084992823\n",
      "Epoch 20, Loss: 0.0017234040056842142\n",
      "Epoch 30, Loss: 0.0005622783917239001\n",
      "Epoch 40, Loss: 0.0003518145625641624\n",
      "Epoch 50, Loss: 0.00023961817307730453\n",
      "Epoch 60, Loss: 0.0001830723762618725\n",
      "Epoch 70, Loss: 0.00014657485686343552\n",
      "Epoch 80, Loss: 0.000123269893411049\n",
      "Epoch 90, Loss: 0.00010412333337141243\n",
      "CPU times: total: 1min 59s\n",
      "Wall time: 4min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train(x_train, y_train, weights, biases, epochs=100, learning_rate=1e-3, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9304e50f-f24d-4836-866a-21f076597c72",
   "metadata": {},
   "source": [
    "**Testing the neural network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c83843f4-5a25-47bc-8622-ee31f2dd0096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 0.9791, Loss: 0.1139088547405424\n"
     ]
    }
   ],
   "source": [
    "def test(X, Y, weights, biases):\n",
    "    _, activations = forward_pass(X, weights, biases)\n",
    "    \n",
    "    loss = cross_entropy_loss(Y, activations[-1])\n",
    "    \n",
    "    preds = np.argmax(activations[-1], axis=1)\n",
    "    true_labels = np.argmax(Y, axis=1)\n",
    "    total_correct_preds = np.sum(preds == true_labels)\n",
    "    total_samples = len(true_labels)\n",
    "    accuracy = total_correct_preds / total_samples\n",
    "    \n",
    "    return accuracy, loss\n",
    "\n",
    "accuracy, loss = test(x_test, y_test, weights, biases)\n",
    "\n",
    "print(f'Accuracy on the test set: {accuracy}, Loss: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad60bd6f-899b-424a-8249-9bc075da36ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "### PyTorch implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db30a45d-44ab-4471-b187-0a1f999e5e2c",
   "metadata": {},
   "source": [
    "Let's now implement the same neural network using PyTorch and compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428af077-858e-4ded-8c43-a2424c351f75",
   "metadata": {},
   "source": [
    "**Define the nn architecture and training loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d26d1148-ff92-4c2c-9674-b288a0c2ec9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layer1 = nn.Linear(784, 128)\n",
    "        self.layer2 = nn.Linear(128, 64)\n",
    "        self.layer3 = nn.Linear(64, 32)\n",
    "        self.layer4 = nn.Linear(32, 10)\n",
    "        self.a1 = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.a1(self.layer1(x))\n",
    "        x = self.a1(self.layer2(x))\n",
    "        x = self.a1(self.layer3(x))\n",
    "        x = self.layer4(x)\n",
    "        return x\n",
    "\n",
    "model = MLP()\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "def train_step(train_loader):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(X_batch)\n",
    "        loss = loss_func(outputs, y_batch)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    \n",
    "    return avg_train_loss\n",
    "\n",
    "def train_loop(train_loader, num_epochs=100):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_step(train_loader)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {train_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d72d424-61d2-48bf-877f-47535e136ba1",
   "metadata": {},
   "source": [
    "**Loading and preprocessing the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c340558-22fc-4c28-b910-b201da157922",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(x_train.shape[0], -1) / 255\n",
    "x_test = x_test.reshape(x_test.shape[0], -1) / 255\n",
    "\n",
    "\n",
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b71cfd0-1de1-4d9f-aca2-9bcf605c8e4c",
   "metadata": {},
   "source": [
    "**Training the neural network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77e75595-2fd3-471f-9622-bba3209d1403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.3026800039927164\n",
      "Epoch 10, Loss: 0.6369109269618988\n",
      "Epoch 20, Loss: 0.37667592872778577\n",
      "Epoch 30, Loss: 0.30402979166110355\n",
      "Epoch 40, Loss: 0.24942544237176578\n",
      "Epoch 50, Loss: 0.20505223579903445\n",
      "Epoch 60, Loss: 0.17163274384935698\n",
      "Epoch 70, Loss: 0.146643717293938\n",
      "Epoch 80, Loss: 0.1276044616724054\n",
      "Epoch 90, Loss: 0.1130687143864731\n",
      "CPU times: total: 15min 10s\n",
      "Wall time: 4min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_loop(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086d1152-59a3-4397-b141-abc130a74d33",
   "metadata": {},
   "source": [
    "**Testing the neural network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2c4fe1c-407f-42c8-a29e-0f89be71e8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 0.9626, Loss: 0.12447559743927726\n"
     ]
    }
   ],
   "source": [
    "def test(test_loader):\n",
    "    model.eval()\n",
    "    total_test_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            outputs = model(X_batch)\n",
    "            loss = loss_func(outputs, y_batch)\n",
    "            total_test_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == y_batch).sum().item()\n",
    "            total_samples += y_batch.size(0)\n",
    "    \n",
    "    avg_test_loss = total_test_loss / len(test_loader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    \n",
    "    return accuracy, avg_test_loss\n",
    "\n",
    "accuracy, loss = test(test_loader)\n",
    "print(f'Accuracy on the test set: {accuracy}, Loss: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82a3044-1f84-472e-a443-2236b67f0bfb",
   "metadata": {},
   "source": [
    "**Saving and loading the models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcc70a1b-0650-44da-b117-786cf9004f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save PyTorch model\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36f98686-0278-49fc-8d78-58ed256344d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load PyTorch model\n",
    "model.load_state_dict(torch.load('model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e84b1345-fdec-432f-98eb-0232e12e1cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save NumPy weights and biases\n",
    "weights_and_biases = {\n",
    "    \"weights\": weights,\n",
    "    \"biases\": biases\n",
    "}\n",
    "\n",
    "np.save('weights_and_biases.npy', weights_and_biases, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c941adae-4168-4f9e-8472-88f24b91fd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the weights and biases from the saved numpy file\n",
    "loaded_weights_and_biases = np.load('weights_and_biases.npy', allow_pickle=True).item()\n",
    "\n",
    "# Extract weights and biases\n",
    "weights = loaded_weights_and_biases['weights']\n",
    "biases = loaded_weights_and_biases['biases']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681b99b9-f87c-494e-b525-e4225ecdd535",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed52823c-b1a2-461a-8e65-42e572462f5d",
   "metadata": {},
   "source": [
    "The main goal of this project was to implement a simple neural network for the MNIST classification task using just NumPy, and compare it to a PyTorch implementation.\n",
    "Both neural networks achieved high performance on the test set:\n",
    "\n",
    "**Results**\n",
    "- **NumPy Implementation**:\n",
    "    - Accuracy: 97.91%\n",
    "    - Loss: 0.1139\n",
    "    - Training time: 4min 38s\n",
    "    \n",
    "- **PyTorch Implementation**:\n",
    "    - Accuracy: 96.26%\n",
    "    - Loss: 0.1245\n",
    "    - Training time: 4min 32s\n",
    "\n",
    "The NumPy nn slightly outperformed the PyTorch nn, which may be due to different initialization methods, differences in optimization routines or randomness in training. However, training times are almost the same."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
